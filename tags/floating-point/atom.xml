<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title>Sharif&#x27;s Page - floating point</title>
	<subtitle>Sharif Haason&#x27;s personal website for various notes and ideas</subtitle>
	<link href="https://sharifhsn.github.io/tags/floating-point/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://sharifhsn.github.io/"/>
	<generator uri="https://www.getzola.org/">Zola</generator>
	<updated>2022-05-31T00:00:00+00:00</updated>
	<id>https://sharifhsn.github.io/tags/floating-point/atom.xml</id>
	<entry xml:lang="en">
		<title>Floating Point Architecture</title>
		<published>2022-05-31T00:00:00+00:00</published>
		<updated>2022-05-31T00:00:00+00:00</updated>
		<link href="https://sharifhsn.github.io/floating-point-architecture/" type="text/html"/>
		<id>https://sharifhsn.github.io/floating-point-architecture/</id>
		<content type="html">&lt;p&gt;Floating point numbers are complicated enough in the abstract. How do systems programmers and hardware designers cope with the complexity of floating point?&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;h2 id=&quot;floating-point-instructions&quot;&gt;Floating Point Instructions&lt;&#x2F;h2&gt;
&lt;p&gt;In the general computer architecture, CPUs function by executing a stream of instructions. The set of instructions is defined by the hardware model, such as x86 or ARM. Instructions are typically tiny operations that need to be done extremely often. Some examples of instructions are &lt;code&gt;mov&lt;&#x2F;code&gt; to move values in and out of memory, &lt;code&gt;sub&lt;&#x2F;code&gt; to subtract numbers in registers, etc. In order to speed up floating point operations, hardware can include floating point instructions instead of letting software handle it.&lt;&#x2F;p&gt;
&lt;p&gt;Most of the time, instruction sets only allow floating point operations in the same precision. However, there is a useful optimization that is not often implemented in hardware. A hypothetical instruction to multiply two single precision numbers that results in a double precision number would be very useful. In particular, it would enable for the use of &lt;em&gt;iterative improvement&lt;&#x2F;em&gt; algorithms that would otherwise have compounding catastrophic cancellations if the precision was not increased temporarily.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;subexpression-evaluation&quot;&gt;Subexpression Evaluation&lt;&#x2F;h2&gt;
&lt;p&gt;The order of operations is very important for preserving invariants in floating point. For example, floating point numbers do not necessarily follow the associative property, so compilers that optimize out parentheses will exhibit undefined behavior.&lt;&#x2F;p&gt;
&lt;p&gt;This can also cause issues when numbers with different precisions are operated on. There are two solutions to this problem. One is the solution that languages like OCaml take where all values in an arithmetic expression must have the same type. This means that integers will never be implicitly cast to floating point when they are operated with them, because they are not allowed to be operated with them anyway. However, this can be an overly strict restriction on the types of programs that can be written.&lt;&#x2F;p&gt;
&lt;p&gt;Another solution is the one that C takes, which is to establish rules for subexpression evaluation while allowing mixed type expressions. K&amp;amp;R C requires that every operation be done in double precision, but this can cause incompatibility between expressions and a stored value that are in different precisions.&lt;&#x2F;p&gt;
&lt;p&gt;A better way to do subexpression evaluation is to set a tentative precision for each expression, then increase the precision as needed in the wider expression. However, this means that the precision of a sub-expression can change with the expression it is embedded in, which is potentially unexpected behavior for a programmer.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ieee-conformation&quot;&gt;IEEE Conformation&lt;&#x2F;h2&gt;
&lt;p&gt;Languages like C conform to the IEEE specification, but the ways in which they do it can cause issues in implementation. If the exact rounding operation is implemented in hardware, then all the language has to do is call the appropriate instruction. &lt;&#x2F;p&gt;
&lt;p&gt;However, there are some aspects of floating point that are harder to represent.  Floating point has a certain &lt;em&gt;state&lt;&#x2F;em&gt; associated with it, with rounding mode, flags, trap handlers, etc. This state must be read and written to, and must be preserved across subroutines.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;NaN&lt;&#x2F;code&gt;s also cause issue. The reflexive property is typically taken as invariant since it is in integer math, but it can be disastrous with &lt;code&gt;NaN&lt;&#x2F;code&gt;s. One massive consequence of the inclusion of &lt;code&gt;NaN&lt;&#x2F;code&gt;s is that floating point numbers &lt;em&gt;cannot have a total order&lt;&#x2F;em&gt;. This is because &lt;code&gt;NaN&lt;&#x2F;code&gt;s are explicitly unordered with respect to other floating point numbers. This has cascading consequences for the implementation of comparison operators like &amp;lt;, &amp;gt;, and =.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;optimization&quot;&gt;&amp;quot;Optimization&amp;quot;&lt;&#x2F;h2&gt;
&lt;p&gt;Compilers make many optimizations to programs in order to increase performance or reduce instruction count. These optimizations are usually made with certain invariants in mind so that the accuracy of the program is not affected. However, because floating point has different invariants than integer math, compilers can make mistakes when optimizing floating point math. For example,&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;c&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-c &quot;&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;float&lt;&#x2F;span&gt;&lt;span&gt; ε = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;do &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    ε = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;0.5 &lt;&#x2F;span&gt;&lt;span&gt;* ε;
&lt;&#x2F;span&gt;&lt;span&gt;} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b48ead;&quot;&gt;while &lt;&#x2F;span&gt;&lt;span&gt;(ε + &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;will estimate \(ε\). However, a compiler might notice that &lt;code&gt;ε + 1 &amp;gt; 1&lt;&#x2F;code&gt; for integer math is equivalent to &lt;code&gt;ε &amp;gt; 0&lt;&#x2F;code&gt; and make that optimization, without considering that the expression \(ε ⊕ 1\) has a special meaning that is different for floating point numbers than a test for positivity.&lt;&#x2F;p&gt;
&lt;p&gt;In general, many algorithms with floating point will exhibit expressions that, upon first blush, seem to be redundant in integer math. Having consideration for the ways in which floating point math can change values is important.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;exceptions&quot;&gt;Exceptions&lt;&#x2F;h2&gt;
&lt;p&gt;Trap handlers are empowered to be able to access variables in programs. However, computers which have parallel arithmetic may not necessarily be able to easily identify which operation threw an exception. Trap handlers must also be able to identify programs.&lt;&#x2F;p&gt;
&lt;p&gt;The reordering of certain instructions can also cause issues. Compilers will often change the order of certain arithmetic instructions when they are exact so that the semantics don&#x27;t change. However, if the operation traps, then it is not as easy to identify the operation that trapped, since the operation that happened in parallel will modify the trapped arithmetic.&lt;&#x2F;p&gt;
&lt;p&gt;One solution to this is &lt;em&gt;presubstitution&lt;&#x2F;em&gt; where a user that knows that an exception can create their own handler for an exception and substitute the value themselves beforehand. However, because it goes against IEEE-754, it&#x27;s unlikely to be proliferated.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;All this information comes from the landmark paper &lt;a href=&quot;https:&#x2F;&#x2F;docs.oracle.com&#x2F;cd&#x2F;E19957-01&#x2F;800-7895&#x2F;800-7895.pdf&quot;&gt;What Every Computer Scientist Should Know About Floating-Point Arithmetic&lt;&#x2F;a&gt;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>IEEE-754</title>
		<published>2022-05-31T00:00:00+00:00</published>
		<updated>2022-05-31T00:00:00+00:00</updated>
		<link href="https://sharifhsn.github.io/ieee-754/" type="text/html"/>
		<id>https://sharifhsn.github.io/ieee-754/</id>
		<content type="html">&lt;p&gt;&lt;strong&gt;IEEE-754&lt;&#x2F;strong&gt; is the standard for floating point computation around the world. Thus, any real-world discussion of floating point must include it.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;h2 id=&quot;format&quot;&gt;Format&lt;&#x2F;h2&gt;
&lt;p&gt;IEEE-754 floating point numbers are always in base 2 binary; this makes sense as it minimizes wobble which scales with \(β\). It also allows for a special optimization that can add an extra bit of precision without increasing the size of the number. To demonstrate that, we can look at the number \(001010.0011\). The normalized floating point representation of this is \(1.0100011 × 2^3\). Notice how the digit in front of the decimal point is always 1. It can&#x27;t be 0, because then we could just shift the decimal point and throw away the zero as we did with the initial number. Therefore, IEEE-754 assumes that the first bit is always 1 and only encodes the bits after the decimal point, granting an extra 1. There is an obvious problem with this: the number 0 cannot be represented. In IEEE-754, if the significand and exponent are all 0, then the number is the special case of 0.&lt;&#x2F;p&gt;
&lt;p&gt;IEEE-754 allows for four possible precisions: single, double, single-extended, and double-extended:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Parameter&lt;&#x2F;th&gt;&lt;th&gt;Single&lt;&#x2F;th&gt;&lt;th&gt;Single-Extended&lt;&#x2F;th&gt;&lt;th&gt;Double&lt;&#x2F;th&gt;&lt;th&gt;Double-Extended&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;\(p\)&lt;&#x2F;td&gt;&lt;td&gt;24&lt;&#x2F;td&gt;&lt;td&gt;32&lt;&#x2F;td&gt;&lt;td&gt;53&lt;&#x2F;td&gt;&lt;td&gt;64&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;\(e_{max}\)&lt;&#x2F;td&gt;&lt;td&gt;+127&lt;&#x2F;td&gt;&lt;td&gt;+1023&lt;&#x2F;td&gt;&lt;td&gt;+1023&lt;&#x2F;td&gt;&lt;td&gt;+16383&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;\(e_{min}\)&lt;&#x2F;td&gt;&lt;td&gt;-126&lt;&#x2F;td&gt;&lt;td&gt;-1022&lt;&#x2F;td&gt;&lt;td&gt;-1022&lt;&#x2F;td&gt;&lt;td&gt;-16382&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bits for exponent&lt;&#x2F;td&gt;&lt;td&gt;8&lt;&#x2F;td&gt;&lt;td&gt;11&lt;&#x2F;td&gt;&lt;td&gt;11&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;total # of bits&lt;&#x2F;td&gt;&lt;td&gt;32&lt;&#x2F;td&gt;&lt;td&gt;43&lt;&#x2F;td&gt;&lt;td&gt;64&lt;&#x2F;td&gt;&lt;td&gt;79&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The usefulness of extended precision is that it grants a large amount of guard digits for internal calculations, for example in a calculator. There are fast algorithms for common transcendental functions like \(\log\), but most of them have a large amount of possible error. By using extended precision internally then rounding to regular precision on display, calculators can quickly compute transcendental functions for users accurately.&lt;&#x2F;p&gt;
&lt;p&gt;The exponent value is calculated using a &lt;em&gt;bias&lt;&#x2F;em&gt; so it can represent negative values. The initial unbiased exponent is subtracted by \(2^{e - 1} - 1\), e.g. 127 for single precision. The reason that the \(e_max\) is always bigger than \(e_min\) is to prevent overflow when the reciprocal of the smallest numbers are taken. In turn, the reciprocal of the largest numbers will create underflow. In general, underflow is preferable to overflow, for reasons that will be clear when we discuss overflow in more depth.&lt;&#x2F;p&gt;
&lt;p&gt;Operations between floating point numbers in IEEE-754 are always exactly rounded. Having a guard digit can reduce error, but it does not guarantee the same result as exact rounding, so it cannot be used. There are methods using two guard digits and a &amp;quot;sticky&amp;quot; bit that can be used for efficient exact rounding, however. Exact rounding is important so that the result of two operations can always be guaranteed to be the same, regardless of hardware. The operations defined under IEEE to be exactly rounded are +, -, ×, &#x2F;, √, %, and conversion with integers. Conversion between binary and decimal is &lt;em&gt;not&lt;&#x2F;em&gt; necessarily exactly rounded, because the most efficient conversion algorithms are not exactly rounded. Transcendental functions are also not specified as exactly rounded because there is no efficient algorithm that works across all hardware.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;nan-and&quot;&gt;&lt;code&gt;NaN&lt;&#x2F;code&gt; and ∞&lt;&#x2F;h2&gt;
&lt;p&gt;If you&#x27;ve used a calculator before, chances are you have seen both of these terms before. But what do they mean in the context of floating point?&lt;&#x2F;p&gt;
&lt;p&gt;Not all bit patterns in IEEE-754 follow the rules that were laid out earlier for significands and exponents. Some aren&#x27;t even valid floating point numbers! There are special values that are possible:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Exponent&lt;&#x2F;th&gt;&lt;th&gt;Fraction&lt;&#x2F;th&gt;&lt;th&gt;Represents&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;\(e = e_{min} - 1\)&lt;&#x2F;td&gt;&lt;td&gt;f = 0&lt;&#x2F;td&gt;&lt;td&gt;±0&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;\(e = e_{min} - 1\)&lt;&#x2F;td&gt;&lt;td&gt;f ≠ 0&lt;&#x2F;td&gt;&lt;td&gt;\(0.f × 2^{e_{min}}\) (&lt;strong&gt;denormalized&lt;&#x2F;strong&gt;)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;\(e_{min} ≤ e ≤ e_{max}\)&lt;&#x2F;td&gt;&lt;td&gt;—&lt;&#x2F;td&gt;&lt;td&gt;\(1.f × 2^e\) (&lt;strong&gt;normalized&lt;&#x2F;strong&gt;)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;\(e = e_{max} + 1\)&lt;&#x2F;td&gt;&lt;td&gt;f = 0&lt;&#x2F;td&gt;&lt;td&gt;±∞&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;\(e = e_{max} + 1\)&lt;&#x2F;td&gt;&lt;td&gt;f ≠ 0&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;NaN&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The middle row represents normalized numbers, which we are already familiar with. We will discuss each of the other special values.&lt;&#x2F;p&gt;
&lt;p&gt;As mentioned earlier, 0 is an exception where the significand and exponent are both 0. But wait, why can 0 be positive &lt;em&gt;and&lt;&#x2F;em&gt; negative? IEEE-754 also defines that \(+0 = -0\) so the distinction seems useless. The reason both are allowed is because it preserves the sign of infinity when they interact. It also allows for functions that are only defined for positive or negative numbers to be able to safely include 0 in them.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Denormalized numbers&lt;&#x2F;em&gt; are numbers with 0 as the hidden bit instead of 1. The reason they need to exist and break these rules is because small numbers being subtracted can end up underflowing to 0. It breaks a common invariant in code that the difference between two unequal numbers is not 0, and can cause bugs. Denormalized numbers gradually underflow to 0 so this problem does not occur. It also significantly reduces relative error for very small numbers.&lt;&#x2F;p&gt;
&lt;p&gt;The idea of infinity is an important one to measure for floating point. Unlike with integer arithmetic, division by zero is not an immediate error which aborts the operation. Rather, overflow can be an expected result which is handled for by using infinity. If the infinity ends up in a denominator, then the computation can underflow to 0 which can be an expected result.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;NaN&lt;&#x2F;code&gt; stands for &amp;quot;Not a Number&amp;quot; and is perhaps the most unique special value&lt;code&gt;NaN&lt;&#x2F;code&gt; does not represent any computable value, and it is &amp;quot;infectious&amp;quot;; any operation with&lt;code&gt;NaN&lt;&#x2F;code&gt; will result in an &lt;code&gt;NaN&lt;&#x2F;code&gt; and &lt;code&gt;NaN&lt;&#x2F;code&gt;s always compare to false, even with other &lt;code&gt;NaNs&lt;&#x2F;code&gt;. They are a result of computations like \(0&#x2F;0\) or \(\sqrt{-1}\) which are not well-defined in real numbers, and also cannot be represented by infinity. These are all the operations that will produce an &lt;code&gt;NaN&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Operation&lt;&#x2F;th&gt;&lt;th&gt;Production&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;+&lt;&#x2F;td&gt;&lt;td&gt;\(∞ + (-∞)\)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;×&lt;&#x2F;td&gt;&lt;td&gt;\(0 × ∞\)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&#x2F;&lt;&#x2F;td&gt;&lt;td&gt;\(0&#x2F;0\), \(∞&#x2F;∞\)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;%&lt;&#x2F;td&gt;&lt;td&gt;\(x % 0\), \(∞ % y\)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;√&lt;&#x2F;td&gt;&lt;td&gt;\(\sqrt{x}\) for \(x &amp;lt; 0\)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Unlike with denormalized numbers, the value in the significand does not necessarily represent any specific values. Often, some information about the operation will be placed in the significand as a signal. If an operation is done between a real number and an &lt;code&gt;NaN&lt;&#x2F;code&gt;, it will not change the significand of the &lt;code&gt;NaN&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;exceptions&quot;&gt;Exceptions&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;code&gt;NaN&lt;&#x2F;code&gt; and ∞ allow floating point calculations to continue in the face of special circumstances without aborting immediately. However, this behavior is not always appropriate. Implementations of IEEE-754 typically have &lt;strong&gt;trap handlers&lt;&#x2F;strong&gt; that will handle &lt;strong&gt;exceptions&lt;&#x2F;strong&gt; generated by such circumstances. There are five classes of exception that can each be set by a status flag: overflow, underflow, division by zero, invalid operations, and inexact operations. Invalid operations are any that involve &lt;code&gt;NaN&lt;&#x2F;code&gt; except for those when one of the operands are already &lt;code&gt;NaN&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Operations in IEEE-754 must be exact, so if an operation is performed that is inexact, it will raise an exception. However, this raises an issue. Inexact exceptions can happen extremely often, and telling the OS to summon the trap handler every time harms performance. In order to prevent this, a software flag is typically enabled upon inexact exception to mask off future exceptions until the flag is reset.&lt;&#x2F;p&gt;
&lt;p&gt;Although trap handlers can abort the algorithm, they can also exhibit other behavior which can aid algorithms in being more efficient and accurate. For example, IEEE-754 will wrap around overflowing numbers by dividing the computed result by \(2^α\). \(α\) is 192 for single precision and 1536 for double precision. This is useful in partial products when the operation might overflow and underflow at several points. By allowing the products to continue without aborting, the final product might cancel out under&#x2F;overflows and be in range without having to stop the OS at every point.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;rounding&quot;&gt;Rounding&lt;&#x2F;h2&gt;
&lt;p&gt;We stated earlier that round to even is the best and most commonly accepted method of rounding. This is the default mode of IEEE-754. However, there are other acceptable rounding modes that can be enabled for certain operations. These are round to 0, to +∞, and to -∞. These modes turn rounding into floor&#x2F;ceiling operations, which are useful for computing intervals.&lt;&#x2F;p&gt;
&lt;p&gt;We can represent floating point results as an interval between two numbers where the first is rounded to -∞ and the second is rounded to +∞. The exact result is somewhere between those two numbers. This representation can be useful to get an idea of how wide your error is when calculating a result. If the result with single precision has a very wide interval, then you might redo the result in double precision and so on to shrink the interval to some acceptable level of error.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;All this information comes from the landmark paper &lt;a href=&quot;https:&#x2F;&#x2F;docs.oracle.com&#x2F;cd&#x2F;E19957-01&#x2F;800-7895&#x2F;800-7895.pdf&quot;&gt;What Every Computer Scientist Should Know About Floating-Point Arithmetic&lt;&#x2F;a&gt;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Floating Point</title>
		<published>2022-05-28T00:00:00+00:00</published>
		<updated>2022-05-28T00:00:00+00:00</updated>
		<link href="https://sharifhsn.github.io/floating-point/" type="text/html"/>
		<id>https://sharifhsn.github.io/floating-point/</id>
		<content type="html">&lt;p&gt;&lt;strong&gt;Floating point representation&lt;&#x2F;strong&gt; is a complex topic full of nuance. To understand it, we must focus on the abstract method used to represent non-integer numbers.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;h2 id=&quot;numbers-in-computers&quot;&gt;Numbers in Computers&lt;&#x2F;h2&gt;
&lt;p&gt;Math is infinite. Or at least it can be. Numbers like \(5\) can be represented finitely, but a number like \(π\) is not so simple to represent. \(3.14159265358979\dots\) but that still isn&#x27;t enough. It&#x27;ll never be enough.&lt;&#x2F;p&gt;
&lt;p&gt;Unfortunately, computers are not infinite. Although we would like to hold every digit of \(π\) in our computer&#x27;s memory, the fact is that there will always be limits to how numbers can be represented in computers.&lt;&#x2F;p&gt;
&lt;p&gt;Integers are numbers like \(3\), \(15\), \(-154\), etc. They have no decimal point and can be negative. Representing these numbers in a computer is fairly simple, with the limit only being placed on the size of the number.&lt;&#x2F;p&gt;
&lt;p&gt;However, not all real numbers play so nicely. How do we represent the value \(12.5\) in memory? We can&#x27;t have half bits. Can we go infinitely precise on decimals, like \(7.2028301324\)? What about the size? How do we set these bounds?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;floating-point&quot;&gt;Floating Point&lt;&#x2F;h2&gt;
&lt;p&gt;We have to have a different standard to define how these numbers will be represented in a computer. This standard is called &lt;strong&gt;floating point&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Generally, a floating point representation is composed of three parts: the &lt;strong&gt;base&lt;&#x2F;strong&gt; \(β\), the &lt;strong&gt;significand&lt;&#x2F;strong&gt; \(p\), and the largest&#x2F;smallest allowed exponents \(e_{max}\) and \(e_{min}\). This is the complete formula for floating point (ignoring specific details):&lt;&#x2F;p&gt;
&lt;p&gt;$$
\lceil \log_2{(e_{max} - e_{min} + 1)} \rceil + \lceil \log_2{β^p \rceil + 1}
$$&lt;&#x2F;p&gt;
&lt;p&gt;For example, let&#x27;s say we had a floating point representation in base 2 which allowed for exponents of sizes up to \(+128\) and down to \(-127\), and had twenty-three bits of precision for the significand. This representation would need 32 bits.&lt;&#x2F;p&gt;
&lt;p&gt;A number written in floating point representation might look like this:&lt;&#x2F;p&gt;
&lt;p&gt;$$
1.010001 × 2^{7}
$$&lt;&#x2F;p&gt;
&lt;p&gt;where the first part is the significand, the base of the exponent is the base, and it&#x27;s raised to its exponent. Floating point numbers are written like scientific notation to be &lt;em&gt;normalized&lt;&#x2F;em&gt;, where there is only one nonzero digit in front of the decimal point—this will become important for IEEE 754.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;imprecision&quot;&gt;Imprecision&lt;&#x2F;h2&gt;
&lt;p&gt;I mentioned bits of precision, which is finite. Some decimal numbers can&#x27;t be represented by a finite amount of precision. For example, the number \(0.3\) is finitely representable in base 10, but becomes \(\overline{1.001} × 2^{-2}\) in binary. This number will &lt;em&gt;never&lt;&#x2F;em&gt; be perfectly represented in binary, so we have to have approximations.&lt;&#x2F;p&gt;
&lt;p&gt;In order to understand how much error we encounter, we must be able to measure it. There are two techniques to measure error in floating point: &lt;code&gt;ulps&lt;&#x2F;code&gt; or &amp;quot;units in the last place&amp;quot; and &lt;em&gt;relative error&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s use an example here, with \(β = 10\) and \(p = 3\). We want to approximate \(2.781828\) to a floating point number with this precision. Since we can only encode 3 digits of precision, we end up with \(2.78 × 10^0\). If we imagine a special decimal point after the precision stops for floating point, then the difference between these two is \(0.1828\). This is difference for &lt;code&gt;ulps&lt;&#x2F;code&gt;. &lt;strong&gt;The closest floating point number can still have a &lt;code&gt;ulps&lt;&#x2F;code&gt; error of up to \(\frac{β}{2} \cdot β^{-p}\). This number is known as the machine epsilon \(ε\).&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Relative error is a familiar concept in most sciences. It is simply the difference between measured and actual, divided by the actual value. In this case, our floating point approximation \(2.78 × 10^0\) is the &amp;quot;measured&amp;quot; value and our real number \(2.781828\) is our actual value. In this case, it is \(0.001828 &#x2F; 2.781828 = 0.0006\). Since this number can be small, it is often expressed in terms of \(ε\). In this example, \(ε = 5 × 10^{-3} = 0.005\) so our relative error is \(0.13ε\).&lt;&#x2F;p&gt;
&lt;p&gt;Since we know the maximum &lt;code&gt;ulp&lt;&#x2F;code&gt; error for the closest floating point number, we should find the same for relative error. One difference about relative error is that it changes based on the size of the numbers, not just the absolute error in the significand. The largest possible error is \(\frac{β}{2}β^{-p} × β^e\).&lt;&#x2F;p&gt;
&lt;p&gt;However, the relative error changes based on the size of the real number in question, and all real numbers within the same exponent range will have this error. So, the relative error for a number closer to \(1.0 × β^e\) will be larger than for a number closer to \(β × β^e\). This variation is called &lt;strong&gt;wobble&lt;&#x2F;strong&gt;. The relative error is always bounded by \(ε\), as in the prior example. Depending on the significand, however, the wobble can be up to \(β\) for the same exponent. Crucially, this wobble can be expressed in either relative error or &lt;code&gt;ulps&lt;&#x2F;code&gt;, as long as the other is held fixed.&lt;&#x2F;p&gt;
&lt;p&gt;Typically, relative error is used, because it is more useful for compounding operations whereas &lt;code&gt;ulps&lt;&#x2F;code&gt; can vary wildly within \(β\).&lt;&#x2F;p&gt;
&lt;p&gt;An important concept to understand here is &lt;strong&gt;contaminated digits&lt;&#x2F;strong&gt;. These are the least significant digits of a floating point number which may be error-prone and are therefore untrustworthy. The number of contaminated digits is \(\log_β{n}\) where \(n\) is the factor of the relative error to \(ε\), like \(0.13\) in the earlier example.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;error-mitigation&quot;&gt;Error Mitigation&lt;&#x2F;h2&gt;
&lt;p&gt;We have found how to measure error; now how do we reduce it? One method is to use &lt;strong&gt;guard digits&lt;&#x2F;strong&gt;. When performing calculations, a computer can &amp;quot;extend&amp;quot; the calculation by a few digits so that the extended digits are contaminated by the floating point operation. These digits will then be rounded out of the final result, reducing the contamination of the result.&lt;&#x2F;p&gt;
&lt;p&gt;Without a guard digit, the relative error can be large as \(β - 1\), which can put every digit in error! However, adding just &lt;em&gt;one&lt;&#x2F;em&gt; guard digit bounds the relative error to &lt;em&gt;less than \(2ε\)!&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Another method is to reduce the number of &lt;strong&gt;cancellations&lt;&#x2F;strong&gt;. A cancellation occurs when nearby quantities are subtracted. When this happens, the most significant and uncontaminated digits cancel out and the less significant, more contaminated digits are left. Cancellations can seriously magnify rounding errors in a series of cancellations; this is called &lt;em&gt;catastrophic cancellation&lt;&#x2F;em&gt;. However, cancellation can also be &lt;em&gt;benign&lt;&#x2F;em&gt;. If the two quantities being subtracted are exactly known, then the subtraction will have a tiny relative error if done with a guard digit.&lt;&#x2F;p&gt;
&lt;p&gt;Sometimes, we can rearrange formulas to have less or more benign cancellations. For example, the formula \(x^2 - y^2\) has a catastrophic cancellation because \(x^2\) and \(y^2\) both suffer from rounding error. However, this formula can be rearranged to \((x ⊕ y) ⊗ (x ⊖ y)\). The cancellation is now benign because \(x\) and \(y\) are presumably exact values without rounding error yet.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice that the operands in that formula are circled. This is a notation to indicate that these operations are performed by a computer and therefore may accrue rounding error, whereas the ordinary operands are used for exact calculations.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;However, the impact of making a cancellation benign can be limited if the inputs to the equation are already inexact. This is common when converting between decimal and binary numbers. &lt;strong&gt;It is always worth eliminating a cancellation, though.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;exact-rounding&quot;&gt;Exact Rounding&lt;&#x2F;h2&gt;
&lt;p&gt;Although guard digits mitigate error, they will in many cases give a different value than the &lt;strong&gt;exactly rounded&lt;&#x2F;strong&gt; result. That is the result if the floating point number was computed exactly, then rounded to precision. Many algorithms require exact rounding to work properly.&lt;&#x2F;p&gt;
&lt;p&gt;The nature of rounding is controversial. The most commonly accepted rounding mode is &lt;em&gt;round to even&lt;&#x2F;em&gt;. This will round numbers that end in the half digit to whichever direction makes the number even. For example, \(12.5\) will round to \(12\), not \(13\) because \(12\) is even. This achieves the result of rounding up and down being equal chance for the half digit, 5 in this case.&lt;&#x2F;p&gt;
&lt;p&gt;Exact rounding is useful for holding certain invariants in floating point calculation. For example, one way to increase precision in calculations is to split a multiple precision number into an array of single precision numbers. Adding these numbers back together with exact rounding will recover the multiple precision numbers.&lt;&#x2F;p&gt;
&lt;p&gt;We can represent a multiplication of two double precision floating point numbers \(x\) and \(y\) using this method. We can split \(x\) into \(x_h\) and \(x_l\), each of which are single precision, and \(y\) into \(y_h\) and \(y_l\) likewise. Then, we can represent the multiplication as so:&lt;&#x2F;p&gt;
&lt;p&gt;$$
x \cdot y = (x_h + x_l)(y_h + y_l) = x_hy_h + x_hy_l + x_ly_h + x_ly_l
$$&lt;&#x2F;p&gt;
&lt;p&gt;In this way, a multiplication of two double precision numbers can be represented as the sum of multiplications of single precision numbers. Because the numbers are exactly rounded, the number is completely recoverable. This means that double precision multiplication can be possible on a computer which only supports single precision multiplication.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;All this information comes from the landmark paper &lt;a href=&quot;https:&#x2F;&#x2F;docs.oracle.com&#x2F;cd&#x2F;E19957-01&#x2F;800-7895&#x2F;800-7895.pdf&quot;&gt;What Every Computer Scientist Should Know About Floating-Point Arithmetic&lt;&#x2F;a&gt;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
</content>
	</entry>
</feed>
